{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "#### EE-556 Mathematics of Data - Fall 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we consider a multiclass classification task modeled by multinomial (softmax) logistic regression. Your goal will be to analyze the estimator and its properties (convexity, existence/uniqueness), and to derive gradients/Hessians and smoothness bounds. The first part consists of theoretical questions only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  ‚ÑπÔ∏è <strong>Information on group based work:</strong>\n",
    "</div>\n",
    "\n",
    "- You are to deliver only 1 notebook per group.\n",
    "- Asking assistance beyond your group is ok, but answers should be individual to the group.\n",
    "- In the event that there was <span style=\"color: red;\">disproportional work done</span> by different group members, let the TAs know.\n",
    "- Only one member of the group is allowed to use AI. We will require sharing the conversation history with the AI in the form of a public link. If you use multiple conversations across the same or multiple tools please share all of them. Name the person in your group who is allowed to use AI. We encourage you to use the AI to help you understand the material, but we ask you to write the code and theory solutions by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #f00; background-color: #fdd; padding: 10px; border-radius: 5px;\">\n",
    "  ‚ö†Ô∏è Do not forget: Write who are the people in your group as well as their respective SCIPER numbers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Person 1 **Sofija Orlovic**: || Person 1 **SCIPER**:\n",
    "\n",
    "\n",
    "Person 2 **Petar Damjanovic**: || Person 2 **SCIPER**:\n",
    "\n",
    "\n",
    "Person 3 **Marko Stojanovic**: || Person 3 **SCIPER**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #0a0; background-color: #dfd; padding: 10px; border-radius: 5px;\">\n",
    "  üìì Feedback on AI use: Please use the following cell to provide feedback on the AI use in this notebook.\n",
    "  \n",
    "  For example, how useful were the tools to you? Which tools did you use? Did you feel like they helped you understand the material better?\n",
    "</div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiclass Softmax Logistic Regression - 15 Points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now model multiclass classification with classes $c \\in \\{1,\\dots,C\\}$. For each sample $(\\mathbf{a}_i, b_i)$ with $\\mathbf{a}_i \\in \\mathbb{R}^p$ and $b_i \\in \\{1,\\dots,C\\}$, let $\\mathbf{X} = [\\mathbf{x}_1,\\dots,\\mathbf{x}_C] \\in \\mathbb{R}^{p\\times C}$ be the class weight matrix. The softmax model defines\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(b_i = c \\mid \\mathbf{a}_i) = \\frac{\\exp(\\mathbf{a}_i^\\top \\mathbf{x}_c)}{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)}.\n",
    "$$\n",
    "\n",
    "Assume i.i.d. samples $\\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n$. Our goal is to estimate $\\mathbf{X}$ by maximum likelihood (and later with an $\\ell_2$ regularizer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__ (1 point) Show that the negative log-likelihood $f$ can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " f(\\mathbf{X})\n",
    " &= - \\log \\mathbb{P}(b_1,\\dots,b_n\\mid \\mathbf{a}_1,\\dots,\\mathbf{a}_n)\\\\\n",
    " &= \\sum_{i=1}^n \\left[ -\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} + \\log \\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k) \\right].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**  \n",
    "We are given i.i.d. samples $\\{(\\mathbf{a}_i, b_i)\\}_{i=1}^n$.  \n",
    "By independence, the joint probability is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}(b_1, \\dots, b_n \\mid \\mathbf{a}_1, \\dots, \\mathbf{a}_n) \n",
    "&= \\prod_{i=1}^n \\mathbb{P}(b_i \\mid \\mathbf{a}_i).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Taking the negative log-likelihood gives\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\mathbf{X})\n",
    "&= - \\log \\mathbb{P}(b_1, \\dots, b_n \\mid \\mathbf{a}_1, \\dots, \\mathbf{a}_n) \\\\\n",
    "&= - \\sum_{i=1}^n \\log \\mathbb{P}(b_i \\mid \\mathbf{a}_i).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From the softmax model we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}(b_i = c \\mid \\mathbf{a}_i) \n",
    "&= \\frac{\\exp(\\mathbf{a}_i^\\top \\mathbf{x}_c)}{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, for the observed label $b_i$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "- \\log \\mathbb{P}(b_i \\mid \\mathbf{a}_i) \n",
    "&= - \\mathbf{a}_i^\\top \\mathbf{x}_{b_i} \n",
    "  + \\log \\left( \\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k) \\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Summing over all samples yields\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\mathbf{X}) \n",
    "&= \\sum_{i=1}^n \\left[ - \\mathbf{a}_i^\\top \\mathbf{x}_{b_i} \n",
    "  + \\log \\left( \\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k) \\right) \\right]. \\blacktriangle\n",
    "\\end{aligned}  \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ (2 points) Show that $\\mathbf{u} \\mapsto \\log\\!\\left(\\sum_{k=1}^C e^{u_k}\\right)$ is convex on $\\mathbb{R}^C$. Then, show that $f(\\mathbf{X})$ is convex.\n",
    "\n",
    "\n",
    "Hint: use Jensen's inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**  \n",
    "We want to show that the function\n",
    "$$\n",
    "\\phi(\\mathbf{u}) = \\log\\!\\left( \\sum_{k=1}^C e^{u_k} \\right)\n",
    "$$\n",
    "is convex on $\\mathbb{R}^C$, and then deduce that $f(\\mathbf{X})$ is convex.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1. Convexity of $\\phi(\\mathbf{u})$.**\n",
    "\n",
    "For $\\theta \\in [0,1]$ and $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^C$, define\n",
    "$$\n",
    "\\mathbf{w} = \\theta \\mathbf{u} + (1-\\theta)\\mathbf{v}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi(\\mathbf{w})\n",
    "&= \\log \\left( \\sum_{k=1}^C e^{w_k} \\right) \\\\\n",
    "&= \\log \\left( \\sum_{k=1}^C e^{\\theta u_k + (1-\\theta) v_k} \\right) \\\\\n",
    "&= \\log \\left( \\sum_{k=1}^C (e^{u_k})^\\theta (e^{v_k})^{1-\\theta} \\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now recall **H√∂lder‚Äôs inequality**: for nonnegative sequences $(x_k), (y_k)$ and exponents $p,q > 1$ with $\\tfrac{1}{p} + \\tfrac{1}{q} = 1$,\n",
    "$$\n",
    "\\sum_{k=1}^C x_k y_k \\;\\leq\\; \\left( \\sum_{k=1}^C x_k^p \\right)^{1/p}\n",
    "\\left( \\sum_{k=1}^C y_k^q \\right)^{1/q}.\n",
    "$$\n",
    "\n",
    "Take\n",
    "$$\n",
    "x_k = e^{u_k}, \\quad y_k = e^{v_k}, \\quad\n",
    "p = \\tfrac{1}{\\theta}, \\quad q = \\tfrac{1}{1-\\theta}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\sum_{k=1}^C (e^{u_k})^\\theta (e^{v_k})^{1-\\theta}\n",
    "\\;\\leq\\; \\left( \\sum_{k=1}^C e^{u_k} \\right)^\\theta\n",
    "\\left( \\sum_{k=1}^C e^{v_k} \\right)^{1-\\theta}.\n",
    "$$\n",
    "\n",
    "Taking logarithms gives\n",
    "$$\n",
    "\\phi(\\theta \\mathbf{u} + (1-\\theta)\\mathbf{v})\n",
    "\\;\\leq\\; \\theta \\phi(\\mathbf{u}) + (1-\\theta)\\phi(\\mathbf{v}),\n",
    "$$\n",
    "so $\\phi$ is convex on $\\mathbb{R}^C$.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2. Convexity of $f(\\mathbf{X})$.**\n",
    "\n",
    "Recall\n",
    "$$\n",
    "f(\\mathbf{X}) \n",
    "= \\sum_{i=1}^n \\Big[ -\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} \n",
    "+ \\log \\left( \\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k} \\right) \\Big].\n",
    "$$\n",
    "\n",
    "- The term $-\\mathbf{a}_i^\\top \\mathbf{x}_{b_i}$ is linear in $\\mathbf{X}$, hence convex.  \n",
    "- The second term is $\\phi(\\mathbf{u})$ with $\\mathbf{u} = [\\mathbf{a}_i^\\top \\mathbf{x}_1, \\dots, \\mathbf{a}_i^\\top \\mathbf{x}_C]$, which is convex since $\\phi$ is convex and linear maps preserve convexity.  \n",
    "- A nonnegative sum of convex functions is convex.\n",
    "\n",
    "Therefore, $f(\\mathbf{X})$ is convex. $\\blacktriangle$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just established that the negative log-likelihood is a convex function. So in principle, any local minimum of the maximum likelihood estimator\n",
    "$$\n",
    "\\mathbf{X}^\\star_{ML} = \\arg\\min_{\\mathbf{X} \\in \\mathbb{R}^{p\\times C}} f(\\mathbf{X})\n",
    "$$\n",
    "\n",
    "is a global minimum. But does the minimum always exist? We will ponder this question in the following three points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ (1 point) Explain the difference between infima and minima. Give an example of a convex function on $\\mathbb{R}$ that does not attain its infimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**  \n",
    "The **infimum** of a function $f$ over a domain $D$ is the greatest lower bound of the set $\\{f(x) : x \\in D\\}$.  \n",
    "Formally, $\\alpha = \\inf_{x \\in D} f(x)$ if:\n",
    "1. $\\alpha \\leq f(x)$ for all $x \\in D$, and  \n",
    "2. for every $\\varepsilon > 0$, there exists $x \\in D$ such that $f(x) < \\alpha + \\varepsilon$.  \n",
    "\n",
    "Thus, the infimum always exists (possibly equal to $-\\infty$).  \n",
    "\n",
    "The **minimum** of $f$ over $D$ is a value $\\beta$ such that:\n",
    "1. $\\beta = f(x^*)$ for some $x^* \\in D$, and  \n",
    "2. $\\beta \\leq f(x)$ for all $x \\in D$.  \n",
    "\n",
    "Equivalently, a minimum exists if and only if the infimum is attained at some \n",
    "$x^* \\in D$, in which case\n",
    "$$\n",
    "\\min_{x \\in D} f(x) = f(x^*) = \\inf_{x \\in D} f(x).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider the convex function\n",
    "$$\n",
    "f(x) = e^x, \\quad x \\in \\mathbb{R}.\n",
    "$$\n",
    "We have\n",
    "$$\n",
    "\\inf_{x \\in \\mathbb{R}} f(x) = 0,\n",
    "$$\n",
    "but there is no $x \\in \\mathbb{R}$ such that $e^x = 0$.  \n",
    "Therefore, $f(x)$ has an **infimum** but no **minimum**. $\\blacktriangle$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ (1 point) Assume there exists $\\mathbf{X}_0 \\in \\mathbb{R}^{p\\times C}$ such that for all $i$,\n",
    "$$\n",
    "\\mathbf{a}_i^\\top \\mathbf{x}_{0, b_i} - \\max_{k \\neq b_i} \\mathbf{a}_i^\\top \\mathbf{x}_{0,k} > 0.\n",
    "$$\n",
    "This is called one-versus-all complete separation in multiclass settings. Give a geometric interpretation (e.g., for $p=2$) and explain why the name is appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**  \n",
    "For $p = 2$ and $C = 3$, the condition\n",
    "$$\n",
    "\\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i} - \\max_{k \\neq b_i} \\mathbf{a}_i^\\top \\mathbf{x}_{0,k} > 0\n",
    "$$\n",
    "means that for each sample $\\mathbf{a}_i$, the score for its true class $b_i$ is strictly larger than the score for any other class.  \n",
    "\n",
    "Geometrically, this implies that there exists a set of separating lines (in $\\mathbb{R}^2$) such that each class lies entirely within its own region, separated from the others. In other words, every point is projected (via the inner product) most strongly onto the weight vector of its true class.  \n",
    "\n",
    "This situation is called **one-versus-all complete separation** because for each class $c$, there exists a separating hyperplane (line in 2D) that perfectly distinguishes class $c$ from all the others combined. Thus, each class is linearly separable against the union of the remaining classes. $\\blacktriangle$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you should see that it is likely that some datasets satisfy the complete separation assumption. Unfortunately, as you will show next, this can become an obstacle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(e)__ (1 point) In a one-versus-all complete separation setting (as in (d)), prove that $f$ does not attain its minimum. Hint: consider $f(\\alpha \\mathbf{X}_0)$ as $\\alpha \\to +\\infty$ and compare it to $f(\\mathbf{X}_0)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**  \n",
    "Recall the negative log-likelihood\n",
    "$$\n",
    "f(\\mathbf{X}) \n",
    "= \\sum_{i=1}^n \\left[ -\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} \n",
    "+ \\log \\Big( \\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k} \\Big) \\right].\n",
    "$$\n",
    "\n",
    "From part (d), assume there exists $\\mathbf{X}_0$ such that for every $i$,\n",
    "$$\n",
    "\\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i} > \\max_{k \\neq b_i} \\mathbf{a}_i^\\top \\mathbf{x}_{0,k}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1. Scale the separator.**  \n",
    "Consider $\\alpha \\mathbf{X}_0$ with $\\alpha > 0$. For sample $i$, the term inside $f$ becomes\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\alpha \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i}\n",
    "+ \\log \\Big( \\sum_{k=1}^C e^{\\alpha \\mathbf{a}_i^\\top \\mathbf{x}_{0,k}} \\Big).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Factor out the largest exponent, corresponding to the true class $b_i$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&= -\\alpha \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i}\n",
    "+ \\log \\Bigg( e^{\\alpha \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i}}\n",
    "\\Big( 1 + \\sum_{k \\neq b_i} e^{\\alpha(\\mathbf{a}_i^\\top \\mathbf{x}_{0,k} - \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i})} \\Big) \\Bigg).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2. Simplify.**  \n",
    "This equals\n",
    "$$\n",
    "\\log \\Big( 1 + \\sum_{k \\neq b_i} e^{\\alpha(\\mathbf{a}_i^\\top \\mathbf{x}_{0,k} - \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i})} \\Big).\n",
    "$$\n",
    "\n",
    "Since by assumption $\\mathbf{a}_i^\\top \\mathbf{x}_{0,k} - \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i} < 0$ for all $k \\neq b_i$, each exponential term vanishes as $\\alpha \\to +\\infty$.  \n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\lim_{\\alpha \\to +\\infty} \\Big[ -\\alpha \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i}\n",
    "+ \\log \\Big( \\sum_{k=1}^C e^{\\alpha \\mathbf{a}_i^\\top \\mathbf{x}_{0,k}} \\Big) \\Big] = 0.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3. Total loss.**  \n",
    "Summing over all $i$,\n",
    "$$\n",
    "\\lim_{\\alpha \\to +\\infty} f(\\alpha \\mathbf{X}_0) = 0.\n",
    "$$\n",
    "\n",
    "Since $f(\\mathbf{X}) \\geq 0$ always, the infimum of $f$ is $0$.  \n",
    "But this value is never attained at any finite $\\mathbf{X}$, because each term is strictly positive for finite $\\alpha$.  \n",
    "\n",
    "Therefore, in the complete separation setting, **$f$ does not attain its minimum**. $\\blacktriangle$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We resolve this issue by adding a regularizer. Consider the regularized function\n",
    "\n",
    "$$\n",
    " f_\\mu(\\mathbf{X}) = f(\\mathbf{X}) + \\frac{\\mu}{2} \\|\\mathbf{X}\\|_F^2, \\quad \\mu > 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(f)__ (1 point) Show that the gradient with respect to $\\mathbf{X}$ of $f_\\mu$ can be expressed as\n",
    "$$\n",
    " \\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n \\big( \\mathbf{p}_i - \\mathbf{e}_{b_i} \\big) \\mathbf{a}_i^\\top + \\mu \\mathbf{X},\\tag{1}\n",
    "$$\n",
    "where $\\mathbf{e}_{b_i} \\in \\mathbb{R}^C$ is the [one-hot vector](https://en.wikipedia.org/wiki/One-hot) for class $b_i$, $\\mathbf{p}_i \\in \\mathbb{R}^C$ has entries $p_{i,c} = \\mathbb{P}(b_i=c\\mid \\mathbf{a}_i)$ under the softmax model, and $(\\mathbf{p}_i - \\mathbf{e}_{b_i})\\mathbf{a}_i^\\top$ denotes the outer product.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**  \n",
    "We want to compute the gradient of\n",
    "$$\n",
    "f_\\mu(\\mathbf{X}) \n",
    "= \\sum_{i=1}^n \\left[ -\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} \n",
    "+ \\log \\Big( \\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k} \\Big) \\right]\n",
    "+ \\frac{\\mu}{2} \\|\\mathbf{X}\\|_F^2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1. Gradient of the regularizer.**  \n",
    "The Frobenius norm term gives\n",
    "$$\n",
    "\\nabla_{\\mathbf{X}} \\frac{\\mu}{2}\\|\\mathbf{X}\\|_F^2 = \\mu \\mathbf{X}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2. Gradient of the log-sum-exp term.**  \n",
    "For a fixed sample $i$, consider\n",
    "$$\n",
    "g_i(\\mathbf{X}) = -\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} \n",
    "+ \\log \\Big( \\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k} \\Big).\n",
    "$$\n",
    "\n",
    "- Derivative of the first term:\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}_c}\\big(-\\mathbf{a}_i^\\top \\mathbf{x}_{b_i}\\big)\n",
    "= \\begin{cases}\n",
    "- \\mathbf{a}_i, & c = b_i, \\\\\n",
    "0, & c \\neq b_i.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This can be written compactly using the one-hot vector $\\mathbf{e}_{b_i} \\in \\mathbb{R}^C$ as\n",
    "$$\n",
    "- \\mathbf{a}_i \\mathbf{e}_{b_i}^\\top.\n",
    "$$\n",
    "\n",
    "- Derivative of the second term (log-sum-exp):  \n",
    "Define\n",
    "$$\n",
    "p_{i,c} = \\frac{e^{\\mathbf{a}_i^\\top \\mathbf{x}_c}}\n",
    "{\\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k}}, \n",
    "\\quad \\mathbf{p}_i = (p_{i,1}, \\dots, p_{i,C})^\\top.\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}_c} \\log \\Big( \\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k} \\Big) \n",
    "= p_{i,c}\\,\\mathbf{a}_i.\n",
    "$$\n",
    "\n",
    "So the full contribution from sample $i$ is\n",
    "$$\n",
    "\\mathbf{a}_i (\\mathbf{p}_i - \\mathbf{e}_{b_i})^\\top.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3. Combine over all samples.**  \n",
    "Summing over $i = 1,\\dots,n$, we get\n",
    "$$\n",
    "\\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X})\n",
    "= \\sum_{i=1}^n \\mathbf{a}_i (\\mathbf{p}_i - \\mathbf{e}_{b_i})^\\top + \\mu \\mathbf{X}.\n",
    "$$\n",
    "\n",
    "Or equivalently, using the outer product notation $(\\mathbf{p}_i - \\mathbf{e}_{b_i})\\mathbf{a}_i^\\top$,\n",
    "$$\n",
    "\\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X})\n",
    "= \\sum_{i=1}^n (\\mathbf{p}_i - \\mathbf{e}_{b_i}) \\mathbf{a}_i^\\top + \\mu \\mathbf{X}.\n",
    "$$\n",
    "\n",
    "This matches the desired expression. $\\blacktriangle$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(g)__ (1 point) Show that the Hessian of $f_\\mu$ can be written as\n",
    "$$\n",
    " \\nabla^2 f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top) \\otimes \\big( \\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top \\big) + \\mu \\mathbf{I},\\tag{2}\n",
    "$$\n",
    "where $\\otimes$ is the Kronecker product, and $\\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top$ is the softmax Jacobian, which is positive semidefinite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(h)__ (1 point) Show that $f_\\mu$ is $\\mu$-strongly convex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i)__ (1 point) Is it possible for a strongly convex function to not attain its minimum? Justify your reasoning (you may assume the domain is $\\mathbb{R}^{p\\times C}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show that $f_\\mu$ is smooth, i.e., $\\nabla f_\\mu$ is L-Lipschitz with respect to the Frobenius norm, with a simple conservative bound\n",
    "$$\n",
    " L = \\|\\mathbf{A}\\|_F^2 + \\mu.\n",
    "$$\n",
    "where\n",
    "$$\n",
    " \\mathbf{A} = \\begin{bmatrix}\n",
    "  \\leftarrow &  \\mathbf{a}_1^\\top & \\rightarrow \\\\\n",
    "  \\leftarrow &  \\mathbf{a}_2^\\top & \\rightarrow \\\\\n",
    "   &  \\ldots &  \\\\\n",
    "  \\leftarrow &  \\mathbf{a}_n^\\top & \\rightarrow \\\\\n",
    " \\end{bmatrix}.\n",
    "$$\n",
    "(You may use that the operator norm of the softmax Jacobian is bounded by 1/4, and a looser bound $\\le 1$ is acceptable for grading.)\n",
    "\n",
    "Hint: check the properties of the spectral norm with respect to dot product, Kronecker product, and outer product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 point for all three questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-1)__ Show that $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$, where $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-2)__ Using (2), show that $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{X})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-3)__ Conclude that $f_\\mu$ is $L$-smooth for $L = \\|\\mathbf{A}\\|_F^2 + \\mu$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(l)__ (1 point) KL divergence and NLL. Let $q(b_i\\mid\\mathbf{a}_i)$ be the true label distribution and $p(b_i\\mid\\mathbf{a}_i)$ the model softmax. Write the KL divergence $\\mathrm{KL}(q\\,\\|\\,p)$ and show that minimizing the KL divergence between $q$ and $p$ is equivalent to minimizing the negative log-likelihood derived in (a).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From your work in this section, you have shown that the maximum likelihood estimator for multiclass softmax logistic regression might not exist, but it can be guaranteed to exist by adding a $\\|\\cdot\\|_F^2$ regularizer. Consequently, the estimator for $\\mathbf{X}$ we will use will be the solution of the smooth strongly convex problem,\n",
    "$$\n",
    " \\mathbf{X}^\\star = \\arg\\min_{\\mathbf{X} \\in \\mathbb{R}^{p\\times C}} f(\\mathbf{X}) + \\frac{\\mu}{2}\\|\\mathbf{X}\\|_F^2.\\tag{3}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary logistic regression (specialization for Part 2)\n",
    "\n",
    "While this part analyzed the multiclass (softmax) setting, in the next exercise we will continue under the simplified two-class case.\n",
    "\n",
    "Let labels be $b_i \\in \\{-1, +1\\}$, features $\\mathbf{a}_i \\in \\mathbb{R}^p$, and weight vector $\\mathbf{x} \\in \\mathbb{R}^p$. Define the sigmoid\n",
    "$$\n",
    "\\sigma(t) = \\frac{1}{1+e^{-t}}.\n",
    "$$\n",
    "Model the conditional distribution as\n",
    "$$\n",
    "\\mathbb{P}(b_i = j \\mid \\mathbf{a}_i) = \\sigma\\big(j\\, \\mathbf{a}_i^\\top \\mathbf{x}\\big), \\quad j \\in \\{-1,+1\\}.\n",
    "$$\n",
    "The likelihood over i.i.d. samples $\\{(\\mathbf{a}_i, b_i)\\}_{i=1}^n$ is\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}) = \\prod_{i=1}^n \\sigma\\big(b_i\\, \\mathbf{a}_i^\\top \\mathbf{x}\\big),\n",
    "$$\n",
    "so the negative log-likelihood is\n",
    "$$\n",
    " f(\\mathbf{x}) = -\\log \\mathcal{L}(\\mathbf{x}) = \\sum_{i=1}^n \\log\\big(1 + e^{-b_i\\, \\mathbf{a}_i^\\top \\mathbf{x}}\\big).\n",
    "$$\n",
    "\n",
    "__(m)__ (1 point) Show that the gradient of the negative log-likelihood is the standard binary logistic regression gradient:\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\sum_{i=1}^n \\big(-b_i\\, \\sigma(-b_i\\, \\mathbf{a}_i^\\top \\mathbf{x})\\big)\\, \\mathbf{a}_i.\n",
    "$$\n",
    "(Hint: use the chain rule and $\\sigma'(t) = \\sigma(t)\\big(1-\\sigma(t)\\big)$.)\n",
    "\n",
    "We will use this binary formulation in Part 2 - First order methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**  \n",
    "We have\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i=1}^n \\log\\!\\big(1 + e^{-b_i \\mathbf{a}_i^\\top \\mathbf{x}}\\big)\n",
    "= \\sum_{i=1}^n \\log\\!\\big(\\sigma(-b_i \\mathbf{a}_i^\\top \\mathbf{x})^{-1}\\big).\n",
    "$$\n",
    "\n",
    "For coordinate $x_j$, let $t_i = -b_i \\mathbf{a}_i^\\top \\mathbf{x}$. Then\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_j}\\log(\\sigma(t_i)^{-1})\n",
    "= -\\frac{\\sigma'(t_i)}{\\sigma(t_i)} \\cdot \\frac{\\partial t_i}{\\partial x_j}.\n",
    "$$\n",
    "\n",
    "Using $\\sigma'(t) = \\sigma(t)(1-\\sigma(t))$ and $\\tfrac{\\partial t_i}{\\partial x_j}=-b_i a_{ij}$,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_j}\n",
    "= \\sum_{i=1}^n \\big(-b_i \\, (1-\\sigma(t_i)) \\, a_{ij}\\big).\n",
    "$$\n",
    "\n",
    "Since $t_i = -b_i \\mathbf{a}_i^\\top \\mathbf{x}$,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_j}\n",
    "= \\sum_{i=1}^n \\big(-b_i \\, \\sigma(-b_i \\mathbf{a}_i^\\top \\mathbf{x}) \\, a_{ij}\\big).\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\nabla f(\\mathbf{x})\n",
    "= \\sum_{i=1}^n \\big(-b_i \\, \\sigma(-b_i \\mathbf{a}_i^\\top \\mathbf{x}) \\, \\mathbf{a}_i\\big). \\;\\blacktriangle\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
